{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZKwR8rI79xWS"},"source":["#Global Setting"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1100,"status":"ok","timestamp":1680264711127,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"oDi90Pyoj_8K"},"outputs":[],"source":["MAX_LEN = 512\n","DUP_LEN = 16\n","DO_TRAIN = True\n","SEED = 42\n","LEARNING_RATE = 5e-5\n","ADAM_EPSILON = 1e-8\n","EPOCHS = 5\n","WARMUP_PROPORTION = 0.1\n","WEIGHT_DECAY = 0.01\n","LOGGING_STEPS = 1000\n","TRAIN_BATCH_SIZE = 8\n","EVAL_BATCH_SIZE = 64\n","DEVICE = \"cuda\"\n","MAX_GRAD_NORM = 1.0\n","EARLYSTOP_PATIENCE = 20\n","TASK_NAME = \"ee\"\n","model_name = \"bert-base-chinese\"\n","MY_MODEL_NAME = \"NCBI_BERT\"\n","OUTPUT_DIR = \"../ncbi_log/\""]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1680264717241,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"WZuCgIn26Xm6"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m/home/ke/Work/NER_KG_Att_2023/notebooks_bk/NCBI_BERT_BASE.ipynb Cell 3\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ke/Work/NER_KG_Att_2023/notebooks_bk/NCBI_BERT_BASE.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ke/Work/NER_KG_Att_2023/notebooks_bk/NCBI_BERT_BASE.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mncbi_disease\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"]}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"ncbi_disease\")"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5714,"status":"ok","timestamp":1680264722953,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"E5dIYiEd70b9","outputId":"c3848fe3-7261-4ad3-ff75-7e14dc1d6a17"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.9/dist-packages (1.26.103)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.103 in /usr/local/lib/python3.9/dist-packages (from boto3) (1.29.103)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3) (0.6.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.103->boto3) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.103->boto3) (1.26.15)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.103->boto3) (1.16.0)\n"]}],"source":["!pip install boto3"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8399,"status":"ok","timestamp":1680264731346,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"dXWl3mDZ87oN","outputId":"760f7a31-26d1-4c0d-f136-baeb7494c121"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: modelzoo-client[transformers] in /usr/local/lib/python3.9/dist-packages (0.15.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from modelzoo-client[transformers]) (8.4.0)\n","Requirement already satisfied: yaspin==0.16.0 in /usr/local/lib/python3.9/dist-packages (from modelzoo-client[transformers]) (0.16.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from modelzoo-client[transformers]) (2.27.1)\n","Requirement already satisfied: click==7.1 in /usr/local/lib/python3.9/dist-packages (from modelzoo-client[transformers]) (7.1)\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.9/dist-packages (from modelzoo-client[transformers]) (1.1.0)\n","Requirement already satisfied: names==0.3.0 in /usr/local/lib/python3.9/dist-packages (from modelzoo-client[transformers]) (0.3.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from modelzoo-client[transformers]) (4.65.0)\n","Requirement already satisfied: colorama==0.4.3 in /usr/local/lib/python3.9/dist-packages (from modelzoo-client[transformers]) (0.4.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from modelzoo-client[transformers]) (1.13.1+cu116)\n","Requirement already satisfied: transformers>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from modelzoo-client[transformers]) (4.27.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=2.10.0->modelzoo-client[transformers]) (0.13.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=2.10.0->modelzoo-client[transformers]) (2022.10.31)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=2.10.0->modelzoo-client[transformers]) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=2.10.0->modelzoo-client[transformers]) (23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=2.10.0->modelzoo-client[transformers]) (3.10.7)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=2.10.0->modelzoo-client[transformers]) (0.13.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=2.10.0->modelzoo-client[transformers]) (6.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->modelzoo-client[transformers]) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->modelzoo-client[transformers]) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->modelzoo-client[transformers]) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->modelzoo-client[transformers]) (2.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->modelzoo-client[transformers]) (4.5.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: subword_nmt in /usr/local/lib/python3.9/dist-packages (0.3.8)\n","Requirement already satisfied: mock in /usr/local/lib/python3.9/dist-packages (from subword_nmt) (5.0.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from subword_nmt) (4.65.0)\n"]}],"source":["! pip install modelzoo-client[transformers]\n","! pip install subword_nmt"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hpam17Rx91Cy"},"source":["#Dataset Class"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680264731346,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"JqIyoiOl6dyY"},"outputs":[],"source":["from cblue.data import EEDataProcessor\n","from cblue.data import EEDataset"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680264731346,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"sn6VPsqd8Fp9"},"outputs":[],"source":["data_processor = EEDataProcessor(root=\"/content/drive/MyDrive/\")"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680264731346,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"WVAhHbZg-Bz5"},"outputs":[],"source":["train_samples = data_processor.get_train_sample()"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680264731347,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"Zrsyig2MgUQt"},"outputs":[],"source":["from transformers import set_seed\n","\n","set_seed(SEED)"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680264731347,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"1-7iqua-3W7j"},"outputs":[],"source":["import torch\n","from transformers import AutoModel, AutoTokenizer, AutoConfig"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7300,"status":"ok","timestamp":1680264738642,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"KELgnGBx416l","outputId":"d1320bf3-cfb6-4583-f953-dd4c16a50b81"},"outputs":[{"name":"stdout","output_type":"stream","text":["DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-chinese/resolve/main/config.json HTTP/1.1\" 200 0\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-chinese/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"]}],"source":["model = AutoModel.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1680264738642,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"i14kxUqf6-QV","outputId":"fd2d72ce-9f17-4240-ff61-1ac66d528411"},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 8020, 126, 8021, 2791, 2147, 5310, 3867, 6084, 1469, 6629, 3011, 1690, 3490, 1057, 868, 711, 1353, 1908, 1355, 868, 2772, 7410, 3780, 2595, 2552, 2791, 1079, 2835, 6819, 2595, 2552, 1220, 6814, 6862, 4638, 3296, 807, 4545, 3791, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.encode_plus(train_samples[\"orig_text\"][0])"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680264738642,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"2q7qN7Bb71ai"},"outputs":[],"source":["import numpy as np\n","\n","class MyEEDataset(EEDataset):\n","  def __init__(\n","    self,\n","    samples,\n","    data_processor,\n","    tokenizer,\n","    mode='train',\n","    max_length=MAX_LEN,\n","    ignore_label=-100,\n","    model_type='bert',\n","    ngram_dict=None\n","  ):\n","    super(MyEEDataset, self).__init__(\n","        samples,\n","        data_processor,\n","        tokenizer,\n","        mode,\n","        max_length,\n","        ignore_label,\n","        model_type,\n","        ngram_dict\n","    )\n","\n","  def __getitem__(self, idx):\n","    text = self.orig_text[idx]\n","    inputs = self.tokenizer.encode_plus(text, max_length=self.max_length, padding='max_length', truncation=True)\n","    if self.mode != \"test\":\n","      label = [self.data_processor.label2id[label_] for label_ in\n","            self.labels[idx].split('\\002')]  # find index from label list\n","      label = ([-100] + label[:self.max_length - 2] + [-100] +\n","            [self.ignore_label] * self.max_length)[:self.max_length]  # use ignore_label padding CLS+label+SEP\n","      return np.array(inputs['input_ids']), np.array(inputs['token_type_ids']), \\\n","          np.array(inputs['attention_mask']), np.array(label)\n","    else:\n","      return np.array(inputs['input_ids']), np.array(inputs['token_type_ids']), \\\n","          np.array(inputs['attention_mask']),"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680264738642,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"X07kWCwi6CtD","outputId":"c36e9ad2-ad48-4ed0-f462-44ffa8f27ccb"},"outputs":[{"data":{"text/plain":["(array([ 101,  125,  119, 5018,  676, 5102, 8020,  122, 8021, 3187, 4568,\n","        4307, 2595,  100, 5341, 1394, 2519, 2642, 5442, 8024, 2399, 7977,\n","        2207,  754,  126, 2259,  511,  102,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0]),\n"," array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0]),\n"," array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0]),\n"," array([-100,    0,    0,    0,    0,    0,    0,    0,    0,    5,    6,\n","           6,    6,    6,    6,    6,    6,    6,    6,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100]))"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset = MyEEDataset(train_samples, data_processor=data_processor, tokenizer=tokenizer, mode='train', max_length=MAX_LEN)\n","train_dataset.__getitem__(2)"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680264738642,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"Jw-FyBOQ-H9w"},"outputs":[],"source":["eval_samples = data_processor.get_dev_sample()\n","dev_dataset = MyEEDataset(eval_samples, data_processor=data_processor, tokenizer=tokenizer, mode='train', max_length=MAX_LEN)"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680264738642,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"_3QO8VEz-ptl"},"outputs":[],"source":["test_samples = data_processor.get_test_sample()\n","test_dataset = MyEEDataset(test_samples, data_processor=data_processor, tokenizer=tokenizer, mode='test', max_length=MAX_LEN)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dsFLt0ky94mW"},"source":["#Model Class"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680264738643,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"Dk2BsOoN05cq"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class CrossAttention(nn.Module):\n","    def __init__(self, d_model):\n","        super(CrossAttention, self).__init__()\n","        self.query = nn.Linear(d_model, d_model)\n","        self.key = nn.Linear(d_model, d_model)\n","        self.value = nn.Linear(d_model, d_model)\n","        self.multihead_attn = nn.MultiheadAttention(d_model, num_heads=1, batch_first=True)\n","\n","    def forward(self, x1, x2):\n","        \"\"\"\n","        x1: shape (batch_size, seq_len1, d_model)\n","        x2: shape (batch_size, seq_len2, d_model)\n","        \"\"\"\n","        # project to queries, keys and values\n","        q = self.query(x1)\n","        k = self.key(x2)\n","        v = self.value(x2)\n","\n","        # compute attention weights and output\n","        attn_output, _ = self.multihead_attn(q, k, v)\n","        output = x1 + attn_output\n","\n","        return output\n"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680264738643,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"Jgpt2Rtl05cq"},"outputs":[],"source":["ontology_text = '药品 药品明细 疗效 疗程 药品推荐 厂商 食物 忌口 忌口原因 不忌口后果 推荐食用 检查 检查科目 检查内容 疾病 医保疾病 非医保疾病 预防措施 治疗方法 症状 疾病症状 缓解手段 疾病并发 科室 科室科目 治疗流程'\n","ontology_input = tokenizer.encode_plus(ontology_text, return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680264738643,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"CePPDNDseY4i","outputId":"9b63bde0-9755-443f-e4ec-33f93b1c34c9"},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[ 101, 5790, 1501, 5790, 1501, 3209, 5301, 4545, 3126, 4545, 4923, 5790,\n","         1501, 2972, 5773, 1322, 1555, 7608, 4289, 2555, 1366, 2555, 1366, 1333,\n","         1728,  679, 2555, 1366, 1400, 3362, 2972, 5773, 7608, 4500, 3466, 3389,\n","         3466, 3389, 4906, 4680, 3466, 3389, 1079, 2159, 4565, 4567, 1278,  924,\n","         4565, 4567, 7478, 1278,  924, 4565, 4567, 7564, 7344, 2974, 3177, 3780,\n","         4545, 3175, 3791, 4568, 4307, 4565, 4567, 4568, 4307, 5353, 6237, 2797,\n","         3667, 4565, 4567, 2400, 1355, 4906, 2147, 4906, 2147, 4906, 4680, 3780,\n","         4545, 3837, 4923,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["ontology_input"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1601,"status":"ok","timestamp":1680264740239,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"rBz4E1OLjC5i","outputId":"cf9a8a9b-f20f-4677-e23c-3f4d9069fe9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-chinese/resolve/main/config.json HTTP/1.1\" 200 0\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from torch import nn\n","\n","from transformers import BertConfig, BertModel\n","\n","import logging\n","import os\n","\n","class NER_Model(nn.Module):\n","  def __init__(self, model_name, num_labels):\n","    super().__init__()\n","\n","    # 输入BERT参数\n","    config = BertConfig.from_pretrained(model_name)\n","    self.bert = BertModel.from_pretrained(model_name, config=config)\n","    classifier_dropout = (\n","      config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","    )\n","    self.dropout = nn.Dropout(classifier_dropout)\n","\n","    # IOB2标签总数（B-人名，I-人名，B-地名，I-地名，…，O）\n","    self.num_labels = num_labels\n","\n","    # 用于将BERT嵌入转换为IOB2标签的线性层\n","    self.classifier = nn.Linear(config.hidden_size, num_labels)\n","\n","    self.cross_att = CrossAttention(config.hidden_size)\n","\n","  def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n","    output = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n","    output = output.last_hidden_state\n","    output = self.dropout(output)\n","    logits = self.classifier(output)\n","\n","    loss = None\n","    if labels is not None: # 输入正确答案标签时\n","      loss_fct = nn.CrossEntropyLoss() # 交叉熵误差\n","      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","    return loss, logits\n","\n","  def save_pretrained(self, save_directory):\n","    \"\"\" Save a model and its configuration file to a directory, so that it\n","        can be re-loaded using the `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n","\n","        Arguments:\n","            save_directory: directory to which to save.\n","    \"\"\"\n","    if os.path.isfile(save_directory):\n","        logging.error(\"Provided path ({}) should be a directory, not a file\".format(save_directory))\n","        return\n","    os.makedirs(save_directory, exist_ok=True)\n","\n","    # Only save the model itself if we are using distributed training\n","    model_to_save = self.module if hasattr(self, \"module\") else self\n","\n","    # If we save using the predefined names, we can load using `from_pretrained`\n","    output_model_file = os.path.join(save_directory, MY_MODEL_NAME + \".bin\")\n","    torch.save(model_to_save.state_dict(), output_model_file)\n","    logging.info(\"Model weights saved in {}\".format(output_model_file))\n","\n","  def load_pretrained(self, save_directory):\n","    output_model_file = os.path.join(save_directory, MY_MODEL_NAME + \".bin\")\n","    best_state_dict = torch.load(output_model_file)\n","    if hasattr(self, \"module\"):\n","      self.module.load_state_dict(best_state_dict)\n","    else:\n","      self.load_state_dict(best_state_dict)\n","    return self\n","\n","model = NER_Model(model_name, num_labels=data_processor.num_labels)"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680264740239,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"D8kf6lSZdvhm","outputId":"7612089e-a378-41ad-d5be-66e6c833b5cc"},"outputs":[{"data":{"text/plain":["(torch.Size([1, 34, 19]), 19)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["dev_text = \"研究证实，细胞减少与肺内病变程度及肺内炎性病变吸收程度密切相关。\"\n","input = tokenizer.encode_plus(dev_text, return_tensors=\"pt\")\n","loss, logits = model(input[\"input_ids\"], token_type_ids=input[\"token_type_ids\"], attention_mask=input[\"attention_mask\"])\n","logits.shape, data_processor.num_labels"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680264740240,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"IDTm8mUmC2gn"},"outputs":[],"source":["import logging\n","import sys\n","logging.basicConfig(\n","    stream=sys.stdout, \n","    level=logging.DEBUG, \n","    force=True) "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kStpfqWK9-Ja"},"source":["#Training"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680264740240,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"JtVQxXleSqze"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from cblue.utils import ProgressBar, seed_everything, TokenRematch\n","from cblue.metrics import ee_metric\n","from cblue.metrics import ee_commit_prediction\n","import logging\n","import os\n","\n","class Trainer(object):\n","    def __init__(\n","      self,\n","      model,\n","      data_processor,\n","      tokenizer,\n","      train_dataset=None,\n","      eval_dataset=None,\n","      ngram_dict=None\n","    ):\n","\n","      self.model = model\n","      self.data_processor = data_processor\n","      self.tokenizer = tokenizer\n","\n","      if train_dataset is not None and isinstance(train_dataset, Dataset):\n","        self.train_dataset = train_dataset\n","\n","      if eval_dataset is not None and isinstance(eval_dataset, Dataset):\n","        self.eval_dataset = eval_dataset\n","      self.ngram_dict = ngram_dict\n","\n","    def train(self):\n","      model = self.model\n","      model.to(DEVICE)\n","\n","      train_dataloader = self.get_train_dataloader()\n","\n","      num_training_steps = len(train_dataloader) * EPOCHS\n","      num_warmup_steps = num_training_steps * WARMUP_PROPORTION\n","      num_examples = len(train_dataloader.dataset)\n","\n","      no_decay = ['bias', 'LayerNorm.weight']\n","      optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","          'weight_decay': WEIGHT_DECAY},\n","        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","          'weight_decay': 0.0}\n","      ]\n","\n","      optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, eps=ADAM_EPSILON)\n","      scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps,\n","                              num_training_steps=num_training_steps)\n","      \n","      seed_everything(SEED)\n","      model.zero_grad()\n","\n","      logging.info(\"***** Running training *****\")\n","      logging.info(\"Num samples %d\", num_examples)\n","      logging.info(\"Num epochs %d\", EPOCHS)\n","      logging.info(\"Num training steps %d\", num_training_steps)\n","      logging.info(\"Num warmup steps %d\", num_warmup_steps)\n","\n","      global_step = 0\n","      best_step = None\n","      best_score = .0\n","      cnt_patience = 0\n","      for i in range(EPOCHS):\n","        logging.info(\"**Epoch %d\", i)\n","        pbar = ProgressBar(n_total=len(train_dataloader), desc='Training')\n","        for step, item in enumerate(train_dataloader):\n","          loss = self.training_step(model, item)\n","\n","          if step % 10 == 0:\n","            pbar(step, {'loss': loss.item()})\n","\n","          if MAX_GRAD_NORM:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n","\n","          optimizer.step()\n","          scheduler.step()\n","          optimizer.zero_grad()\n","\n","          global_step += 1\n","\n","          if LOGGING_STEPS > 0 and global_step % LOGGING_STEPS == 0:\n","            print(\"\")\n","            score = self.evaluate(model)\n","            if score > best_score:\n","              best_score = score\n","              best_step = global_step\n","              cnt_patience = 0\n","              self._save_checkpoint(model, global_step)\n","            else:\n","              cnt_patience += 1\n","              logging.info(\"Earlystopper counter: %s out of %s\", cnt_patience, EARLYSTOP_PATIENCE)\n","              if cnt_patience >= EARLYSTOP_PATIENCE:\n","                break\n","        if cnt_patience >= EARLYSTOP_PATIENCE:\n","          break\n","\n","      logging.info(\"Training Stop! The best step %s: %s\", best_step, best_score)\n","      if DEVICE == 'cuda':\n","          torch.cuda.empty_cache()\n","\n","      self._save_best_checkpoint(best_step=best_step)\n","\n","      return global_step, best_step\n","\n","    def evaluate(self, model):\n","      eval_dataloader = self.get_eval_dataloader()\n","      num_examples = len(eval_dataloader.dataset)\n","\n","      preds = None\n","      eval_labels = None\n","\n","      logging.info(\"***** Running evaluation *****\")\n","      logging.info(\"Num samples %d\", num_examples)\n","      for step, item in enumerate(eval_dataloader):\n","        model.eval()\n","\n","        input_ids = item[0].to(DEVICE)\n","        token_type_ids = item[1].to(DEVICE)\n","        attention_mask = item[2].to(DEVICE)\n","        labels = item[3].to(DEVICE)\n","\n","        with torch.no_grad():\n","          outputs = model(\n","              labels=labels, \n","              input_ids=input_ids,\n","              token_type_ids=token_type_ids,\n","              attention_mask=attention_mask,\n","              )\n","          \n","          # outputs = model(labels=labels, **inputs)\n","          loss, logits = outputs[:2]\n","          # active_index = inputs['attention_mask'].view(-1) == 1\n","          active_index = attention_mask.view(-1) == 1\n","          active_labels = labels.view(-1)[active_index]\n","          logits = logits.argmax(dim=-1)\n","          active_logits = logits.view(-1)[active_index]\n","\n","        if preds is None:\n","          preds = active_logits.detach().cpu().numpy()\n","          eval_labels = active_labels.detach().cpu().numpy()\n","        else:\n","          preds = np.append(preds, active_logits.detach().cpu().numpy(), axis=0)\n","          eval_labels = np.append(eval_labels, active_labels.detach().cpu().numpy(), axis=0)\n","\n","      p, r, f1, _ = ee_metric(preds, eval_labels)\n","      logging.info(\"%s-%s precision: %s - recall: %s - f1 score: %s\", TASK_NAME, MY_MODEL_NAME, p, r, f1)\n","      return f1\n","    \n","    def predict(self, model, test_dataset):\n","        test_dataloader = self.get_test_dataloader(test_dataset)\n","        num_examples = len(test_dataloader.dataset)\n","        model.to(DEVICE)\n","\n","        predictions = []\n","\n","        logging.info(\"***** Running prediction *****\")\n","        logging.info(\"Num samples %d\", num_examples)\n","        pbar = ProgressBar(n_total=len(test_dataloader), desc='Prediction')\n","        for step, item in enumerate(test_dataloader):\n","            model.eval()\n","\n","            input_ids = item[0].to(DEVICE)\n","            token_type_ids = item[1].to(DEVICE)\n","            attention_mask = item[2].to(DEVICE)\n","\n","            with torch.no_grad():\n","              outputs = model(\n","                input_ids=input_ids,\n","                ontology_input_ids=ontology_input[\"input_ids\"],\n","                token_type_ids=token_type_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids_ontology=ontology_input[\"token_type_ids\"]\n","                )\n","\n","              logits = outputs[0].detach()\n","              # active_index = (inputs['attention_mask'] == 1).cpu()\n","              active_index = attention_mask == 1\n","              preds = logits.argmax(dim=-1).cpu()\n","\n","              for i in range(len(active_index)):\n","                predictions.append(preds[i][active_index[i]].tolist())\n","            pbar(step=step, info=\"\")\n","\n","        # test_inputs = [list(text) for text in test_dataset.texts]\n","        test_inputs = test_dataset.texts\n","        predictions = [pred[1:-1] for pred in predictions]\n","        predicts = self.data_processor.extract_result(predictions, test_inputs)\n","        ee_commit_prediction(dataset=test_dataset, preds=predicts, output_dir=OUTPUT_DIR)\n","\n","    def _save_checkpoint(self, model, step):\n","      output_dir = os.path.join(OUTPUT_DIR, f'checkpoint-{MY_MODEL_NAME}-{step}')\n","      if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","      model.save_pretrained(output_dir)\n","      self.tokenizer.save_vocabulary(save_directory=output_dir, filename_prefix=MY_MODEL_NAME)\n","      logging.info('Saving models checkpoint to %s', output_dir)\n","\n","    def _save_best_checkpoint(self, best_step):\n","      model = self.model.load_pretrained(os.path.join(OUTPUT_DIR, f'checkpoint-{MY_MODEL_NAME}-{best_step}'))\n","      model.save_pretrained(OUTPUT_DIR)\n","      self.tokenizer.save_vocabulary(save_directory=OUTPUT_DIR, filename_prefix=MY_MODEL_NAME)\n","      logging.info('Saving models checkpoint to %s', OUTPUT_DIR)\n","\n","    def training_step(self, model, item):\n","      model.train()\n","\n","      input_ids = item[0].to(DEVICE)\n","      token_type_ids = item[1].to(DEVICE)\n","      attention_mask = item[2].to(DEVICE)\n","      labels = item[3].to(DEVICE)\n","\n","      outputs = model(\n","              labels=labels, \n","              input_ids=input_ids,\n","              token_type_ids=token_type_ids,\n","              attention_mask=attention_mask,\n","            )\n","\n","      loss = outputs[0]\n","      loss.backward()\n","\n","      return loss.detach()\n","\n","    def get_train_dataloader(self):\n","      return DataLoader(\n","          self.train_dataset,\n","          batch_size=TRAIN_BATCH_SIZE,\n","          shuffle=True\n","      )\n","\n","    def get_eval_dataloader(self):\n","      return DataLoader(\n","          self.eval_dataset,\n","          batch_size=EVAL_BATCH_SIZE,\n","          shuffle=False\n","      )\n","\n","    def get_test_dataloader(self, test_dataset, batch_size=None):\n","      if not batch_size:\n","          batch_size = EVAL_BATCH_SIZE\n","\n","      return DataLoader(\n","          test_dataset,\n","          batch_size=batch_size,\n","          shuffle=False\n","      )"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680264740240,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"TRj6T_HpZsyp"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    data_processor=data_processor,\n","    tokenizer=tokenizer,\n","    train_dataset=train_dataset,\n","    eval_dataset=dev_dataset,\n","    )"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2276972,"status":"ok","timestamp":1680267017207,"user":{"displayName":"Yuanzhi Ke","userId":"12462805310625462125"},"user_tz":-480},"id":"WbeVUhA9aRwU","outputId":"c02dc93f-68d1-4cb1-f524-52c4cf46b1ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:root:***** Running training *****\n","INFO:root:Num samples 15000\n","INFO:root:Num epochs 5\n","INFO:root:Num training steps 9375\n","INFO:root:Num warmup steps 937\n","INFO:root:**Epoch 0\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 991/1875 [==============>...............] - ETA: 3:04  loss: 0.5752 \n","INFO:root:***** Running evaluation *****\n","INFO:root:Num samples 5000\n","INFO:root:ee-Med_BERT precision: 0.7289741666602232 - recall: 0.7289741666602232 - f1 score: 0.7289741666602232\n","INFO:root:Model weights saved in /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-1000/Med_BERT.bin\n","INFO:root:Saving models checkpoint to /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-1000\n","[Training] 1871/1875 [============================>.] - ETA: 0s  loss: 0.6607 INFO:root:**Epoch 1\n","[Training] 121/1875 [>.............................] - ETA: 6:05  loss: 0.4936 \n","INFO:root:***** Running evaluation *****\n","INFO:root:Num samples 5000\n","INFO:root:ee-Med_BERT precision: 0.757022786845951 - recall: 0.757022786845951 - f1 score: 0.757022786845951\n","INFO:root:Model weights saved in /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-2000/Med_BERT.bin\n","INFO:root:Saving models checkpoint to /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-2000\n","[Training] 1121/1875 [================>.............] - ETA: 3:01  loss: 0.4036 \n","INFO:root:***** Running evaluation *****\n","INFO:root:Num samples 5000\n","INFO:root:ee-Med_BERT precision: 0.7559209458048852 - recall: 0.7559209458048852 - f1 score: 0.7559209458048852\n","INFO:root:Earlystopper counter: 1 out of 20\n","[Training] 1871/1875 [============================>.] - ETA: 0s  loss: 0.5785 INFO:root:**Epoch 2\n","[Training] 241/1875 [==>...........................] - ETA: 5:40  loss: 0.3620 \n","INFO:root:***** Running evaluation *****\n","INFO:root:Num samples 5000\n","INFO:root:ee-Med_BERT precision: 0.7669780173047035 - recall: 0.7669780173047035 - f1 score: 0.7669780173047035\n","INFO:root:Model weights saved in /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-4000/Med_BERT.bin\n","INFO:root:Saving models checkpoint to /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-4000\n","[Training] 1241/1875 [==================>...........] - ETA: 2:30  loss: 0.3454 \n","INFO:root:***** Running evaluation *****\n","INFO:root:Num samples 5000\n","INFO:root:ee-Med_BERT precision: 0.7678942851178003 - recall: 0.7678942851178003 - f1 score: 0.7678942851178003\n","INFO:root:Model weights saved in /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-5000/Med_BERT.bin\n","INFO:root:Saving models checkpoint to /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-5000\n","[Training] 1871/1875 [============================>.] - ETA: 0s  loss: 0.2908 INFO:root:**Epoch 3\n","[Training] 371/1875 [====>.........................] - ETA: 5:13  loss: 0.0709 \n","INFO:root:***** Running evaluation *****\n","INFO:root:Num samples 5000\n","INFO:root:ee-Med_BERT precision: 0.7683620842966388 - recall: 0.7683620842966388 - f1 score: 0.7683620842966388\n","INFO:root:Model weights saved in /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-6000/Med_BERT.bin\n","INFO:root:Saving models checkpoint to /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-6000\n","[Training] 1371/1875 [====================>.........] - ETA: 1:58  loss: 0.4888 \n","INFO:root:***** Running evaluation *****\n","INFO:root:Num samples 5000\n","INFO:root:ee-Med_BERT precision: 0.7711959421320818 - recall: 0.7711959421320818 - f1 score: 0.7711959421320818\n","INFO:root:Model weights saved in /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-7000/Med_BERT.bin\n","INFO:root:Saving models checkpoint to /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-7000\n","[Training] 1871/1875 [============================>.] - ETA: 0s  loss: 0.2317 INFO:root:**Epoch 4\n","[Training] 491/1875 [======>.......................] - ETA: 4:48  loss: 0.4349 \n","INFO:root:***** Running evaluation *****\n","INFO:root:Num samples 5000\n","INFO:root:ee-Med_BERT precision: 0.7694909881001167 - recall: 0.7694909881001167 - f1 score: 0.7694909881001167\n","INFO:root:Earlystopper counter: 1 out of 20\n","[Training] 1491/1875 [======================>.......] - ETA: 1:29  loss: 0.2434 \n","INFO:root:***** Running evaluation *****\n","INFO:root:Num samples 5000\n","INFO:root:ee-Med_BERT precision: 0.771439506993791 - recall: 0.771439506993791 - f1 score: 0.771439506993791\n","INFO:root:Model weights saved in /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-9000/Med_BERT.bin\n","INFO:root:Saving models checkpoint to /content/drive/MyDrive/CMeEE/output/checkpoint-Med_BERT-9000\n","[Training] 1871/1875 [============================>.] - ETA: 0s  loss: 0.2482 INFO:root:Training Stop! The best step 9000: 0.771439506993791\n","INFO:root:Model weights saved in /content/drive/MyDrive/CMeEE/output/Med_BERT.bin\n","INFO:root:Saving models checkpoint to /content/drive/MyDrive/CMeEE/output/\n"]}],"source":["ontology_input.to(DEVICE)\n","global_step, best_step = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.predict(test_dataset=test_dataset, model=model)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
